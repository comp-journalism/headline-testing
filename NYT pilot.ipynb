{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Archive.org crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/28154066/how-to-convert-datetime-to-integer-in-python\n",
    "def to_integer(dt_time):\n",
    "    '''Encode date as monthyear integer, to match archive.org timestamp format'''\n",
    "    return 10000*dt_time.year + 100*dt_time.month + dt_time.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/993358/creating-a-range-of-dates-in-python\n",
    "#Build date range of 1500 days back from 12/30/2017\n",
    "start = datetime.datetime(2017, 12, 30)\n",
    "dates = [start - datetime.timedelta(days=x) for x in range(0, 1500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to integer encoding\n",
    "intdates = [to_integer(date) for date in dates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Capture the point at which A/B testing started\n",
    "not_testing = []\n",
    "for i in intdates:\n",
    "    #Get a snapshot for a given day\n",
    "    r = requests.get('http://archive.org/wayback/available?url=nytimes.com&timestamp={}'.format(str(i)))\n",
    "    #Pull the URL for that snapshot from the metadata the API gives us\n",
    "    arc = r.json()['archived_snapshots']['closest']['url']\n",
    "    #Scrape the HTML of the snapshot\n",
    "    r2 = requests.get(arc)\n",
    "    #Check if the Optimizely script is present. If it is, wait and go to the next day. If not, add it to the list.\n",
    "    if 'optimizely.com' not in r2.text:\n",
    "        print(arc)\n",
    "        not_testing.append(arc)\n",
    "    else:\n",
    "        print(i)\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm looking for the point where a bunch of URLs show up in the `not_testing` list in a row, then calling the day after the latest date in that run the start point for testing. The latest URL in this run was `http://web.archive.org/web/20150805230843/http://www.nytimes.com:80/`, so testing would have started on August 6, 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYT headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm using the NYT archive API to pull headline data by month. Format: `http://api.nytimes.com/svc/archive/v1/{year}/{month}.json?api-key={your-api-key}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = %env KEY\n",
    "\n",
    "def query_range(startmo, startyr, endmo, endyr):\n",
    "    '''Build a range of [month, year] pairs to use for querying the archive API'''\n",
    "    startmos = [[i, startyr] for i in range(startmo, 13)]\n",
    "    middle = [[j, i] for i in range(startyr, endyr+1) if i != startyr and i != endyr for j in range(1, 13)]\n",
    "    endmos = [[i, endyr] for i in range(1, endmo+1)]\n",
    "    allmos = [i for sub in [startmos, middle, endmos] for i in sub]\n",
    "    return allmos\n",
    "\n",
    "def cleaning(entry):\n",
    "    '''Clean the response of the archive API so that data can be uniformly written into a PostgreSQL table'''\n",
    "    #Check for a print headline\n",
    "    try:\n",
    "        entry['print_headline'] = entry['headline']['print_headline']\n",
    "    except Exception:\n",
    "        pass\n",
    "    #Check for a headline\n",
    "    try:\n",
    "        entry['headline'] = entry['headline']['main']\n",
    "    except Exception:\n",
    "        entry['headline'] = str(entry['headline'])\n",
    "    #Check for a byline object\n",
    "    try:\n",
    "        entry['byline'] = str(entry['byline'])\n",
    "    except Exception:\n",
    "        entry['byline'] = ''\n",
    "    #Turn keywords - returned as a list of objects - into a comma-separated string\n",
    "    entry['keywords'] = ','.join(i['value'] for i in entry['keywords'])\n",
    "    #Resolve change over time from 'news_desk' to 'new_desk'\n",
    "    if 'new_desk' in entry:\n",
    "        entry['news_desk'] = entry['new_desk']\n",
    "        entry.pop('new_desk', None)\n",
    "    #Remove multimedia metadata\n",
    "    entry.pop('multimedia', None)\n",
    "    #Remove largely empty blog field\n",
    "    entry.pop('blog', None)\n",
    "    #Remove score - unclear purpose\n",
    "    entry.pop('score', None)\n",
    "    #Remove inconsistently applied URI\n",
    "    entry.pop('uri', None)\n",
    "    return entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Database configuration\n",
    "user = %env USER\n",
    "password = %env PASSWORD\n",
    "db = %env DATABASE\n",
    "engine = create_engine('postgresql://{}:{}@localhost:5432/{}'.format(user, password, db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate range of months from mid-2012 to last month\n",
    "morange = query_range(5, 2012, 9, 2018)\n",
    "\n",
    "for i in morange:\n",
    "    #Keep track of current month\n",
    "    print(i)\n",
    "    #Query archive API for current month\n",
    "    h = requests.get('http://api.nytimes.com/svc/archive/v1/{}/{}.json?api-key={}'.format(i[1], i[0], key))\n",
    "    #Clean results\n",
    "    items = h.json()['response']['docs']\n",
    "    cleaned = [cleaning(i) for i in items]\n",
    "    #Convert results to DataFrame\n",
    "    cdf = pd.DataFrame(cleaned)\n",
    "    #Write DataFrame to PostgreSQL table\n",
    "    cdf.to_sql('results', engine, if_exists = 'append')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/pandas-dev/pandas/issues/12265#issuecomment-181838631\n",
    "#Database configuration\n",
    "user = %env USER\n",
    "password = %env PASSWORD\n",
    "db = %env DATABASE\n",
    "engine_read = create_engine('postgresql://{}:{}@localhost:5432/{}'.format(user, password, db), execution_options=dict(stream_results=True))\n",
    "engine_write = create_engine('postgresql://{}:{}@localhost:5432/{}'.format(user, password, db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full pipeline\n",
    "def pipeline(chunk):\n",
    "    '''Combined processing for all identified variables'''\n",
    "    phrases = ['will make you', \n",
    "               'this is why', \n",
    "               'can we guess', \n",
    "               'only [0-9]+ in',\n",
    "               'the reason is',\n",
    "               'are freaking out',\n",
    "               '[0-9]+ stunning photos',\n",
    "               'tears of joy',\n",
    "               'is what happens',\n",
    "               'make you cry',\n",
    "               'give you goosebumps',\n",
    "               'talking about it',\n",
    "               'is too cute',\n",
    "               'shocked to see',\n",
    "               'melt your heart',\n",
    "               '[0-9]+ things only',\n",
    "               'can\\'t stop laughing',\n",
    "               'top [0-9]+ songs',\n",
    "               'twitter reacts to',\n",
    "               'what happened next']\n",
    "    pattern = '|'.join(phrases)\n",
    "    chunk = chunk[chunk['headline'].str.len() > 0]\n",
    "    hed = chunk['headline'].str\n",
    "    tagged = chunk['headline'].map(lambda x: nlp(x))\n",
    "    chunk['quotes'] = hed.count('\\'|\"')\n",
    "    chunk['numbers'] = hed.count('[0-9]+(,[0-9]+)*')\n",
    "    chunk['numbers_starting'] = hed.split(' ').map(lambda x: x[0]).str.count('[0-9]+')\n",
    "    chunk['social_patterns'] = hed.contains(pattern, case = False)\n",
    "    chunk['avg_word_length'] = hed.split(' ').map(lambda x: sum([len(i) for i in x])/len(x))\n",
    "    chunk['word_count'] = chunk['headline'].map(lambda x: len(x.split(' ')))\n",
    "    chunk['character_count'] = chunk['headline'].map(lambda x: len(x))\n",
    "    chunk['interrogatives'] = tagged.map(lambda x: [i.tag_ for i in x]).map(lambda x: x.count('WP') + x.count('WRB'))\n",
    "    chunk['interrogatives_starting'] = tagged.map(lambda x: [i.tag_ for i in x][0]).str.count('WP|WRB')\n",
    "    chunk['personal_pronouns'] = tagged.map(lambda x: [i.tag_ for i in x]).map(lambda x: x.count('PRP') + x.count('PRP$'))\n",
    "    chunk['personal_pronouns_starting'] = tagged.map(lambda x: [i.tag_ for i in x][0]).str.count('PRP|PRP$')\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chunk data and process\n",
    "n = 1\n",
    "for table in pd.read_sql_query('SELECT * FROM results', engine_read, chunksize=10000):\n",
    "    print(\"chunk {}\".format(n))\n",
    "    processed = pipeline(table)\n",
    "    processed.to_sql('processed', engine_write, if_exists='append')\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual variable definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#H1: Count quotation marks\n",
    "test['headline'].str.count('\\'|\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#H2a: Count of numbers\n",
    "test['headline'].str.count('[0-9]+(,[0-9]+)*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#H2b: Count of numbers starting headlines\n",
    "test['headline'].str.split(' ').map(lambda x: x[0]).str.count('[0-9]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#H3a: Count of interrogatives (WP, WRB) https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "test['headline'].map(lambda x: nlp(x)).map(lambda x: [i.tag_ for i in x]).map(lambda x: x.count('WP') + x.count('WRB'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#H3b: Count of interrogatives starting headlines\n",
    "test['headline'].map(lambda x: nlp(x)).map(lambda x: [i.tag_ for i in x][0]).str.count('WP|WRB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#H4: Count of effective social media phrases https://buzzsumo.com/blog/most-shared-headlines-study/#gs.E5zXvW8\n",
    "phrases = ['will make you', \n",
    "           'this is why', \n",
    "           'can we guess', \n",
    "           'only [0-9]+ in',\n",
    "           'the reason is',\n",
    "           'are freaking out',\n",
    "           '[0-9]+ stunning photos',\n",
    "           'tears of joy',\n",
    "           'is what happens',\n",
    "           'make you cry',\n",
    "           'give you goosebumps',\n",
    "           'talking about it',\n",
    "           'is too cute',\n",
    "           'shocked to see',\n",
    "           'melt your heart',\n",
    "           '[0-9]+ things only',\n",
    "           'can\\'t stop laughing',\n",
    "           'top [0-9]+ songs',\n",
    "           'twitter reacts to',\n",
    "           'what happened next']\n",
    "pattern = '|'.join(phrases)\n",
    "\n",
    "test['headline'].str.contains(pattern, case = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#H6: Average word length\n",
    "test['headline'].str.split(' ').map(lambda x: sum([len(i) for i in x])/len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#H7a: Count of personal/possessive nouns (PRP, PRP$) https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "test['headline'].map(lambda x: nlp(x)).map(lambda x: [i.tag_ for i in x]).map(lambda x: x.count('PRP') + x.count('PRP$'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#H7b: Count of personal/possessive nouns starting headlines\n",
    "test['headline'].map(lambda x: nlp(x)).map(lambda x: [i.tag_ for i in x][0]).str.count('PRP|PRP$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word count\n",
    "test['headline'].map(lambda x: len(x.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Character count\n",
    "test['headline'].map(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#H8: Count of uncommon words\n",
    "'''List of popular words taken from https://github.com/dolph/dictionary/blob/master/popular.txt\n",
    "Disregards cardinal numbers (CD), foreign words (FW), possessive endings (POS), and symbols (SYM/$)'''\n",
    "#This approach doesn't work very well. Entity analysis?\n",
    "words = open('popular.txt').read().splitlines()\n",
    "test['headline'].map(lambda x: nlp(x)) \\\n",
    "    .map(lambda x: [i for i in x if i.tag_ not in ['CD', 'FW', 'POS', 'SYM', '$', '``', ',', ':', 'HYPH', '.']]) \\\n",
    "    .map(lambda x: sum([1 if i.text in words else 0 for i in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#H9: Count of proper nouns (NNP and NNPS) https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "#Entity analysis?\n",
    "test['headline'].map(lambda x: nlp(x)).map(lambda x: [i.tag_ for i in x]).map(lambda x: x.count('NNP') + x.count('NNPS'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#H10: Count of active verbs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#H5: Emotional intensity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermediate processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running all headlines through Spacy NLP, extracting relevant values and storing in separate table with ID. Saves time of having to redo NLP in the future for new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/pandas-dev/pandas/issues/12265#issuecomment-181838631\n",
    "#Database configuration\n",
    "user = %env USER\n",
    "password = %env PASSWORD\n",
    "db = %env DATABASE\n",
    "engine_read = create_engine('postgresql://{}:{}@localhost:5432/{}'.format(user, password, db), execution_options=dict(stream_results=True))\n",
    "engine_write = create_engine('postgresql://{}:{}@localhost:5432/{}'.format(user, password, db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_mapping(entry):\n",
    "    entry = entry[entry['headline'].str.len() > 0]\n",
    "    tagged = entry['headline'].map(lambda x: nlp(x)).map(lambda sentence_pr: {\"text\": ' '.join(i.text for i in sentence_pr),\n",
    "                                                  \"tags\": ' '.join(i.tag_ for i in sentence_pr),\n",
    "                                                  \"entities\": ' '.join(i.text for i in sentence_pr.ents),\n",
    "                                                  \"labels\": ' '.join(i.label_ for i in sentence_pr.ents)})\n",
    "    dft = pd.DataFrame(list(tagged))\n",
    "    dft['_id'] = entry['_id']\n",
    "    return dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chunk data and process\n",
    "n = 1\n",
    "for table in pd.read_sql_query('SELECT * FROM results', engine_read, chunksize=10000):\n",
    "    print(\"chunk {}\".format(n))\n",
    "    processed = tag_mapping(table)\n",
    "    processed.to_sql('nlp', engine_write, if_exists='append')\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:abquant]",
   "language": "python",
   "name": "conda-env-abquant-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
